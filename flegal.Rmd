---
title: "RA 1"
author: "Chrystelle Kiang"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
```

### Set Up
The first goal is to re-create Flegal et al's simulation in "Differential Misclassification Arising from Nondifferential Errors in Exposure Measurement"  

E true exposure values, 100 samples evenly spaced over specified interval  
E' measured exposure values: true + error term, where Error terms ~N with mean 0, "specified" SD  
true exposure values range integers 1,600 - 2,499, high/low cutoff of 2,200  
standard deviation of measurement error varied at 100, 300, 500  

They first look at misclassification close to cutpoints of 2,000 and 2,200. Figure 1 plots the measured exposure values (Y axis) against the true exposure values (X axis) at two cutpoints.

```{r recreate}
pop0 <- tibble(E = as.integer(seq(from = 1600, to = 2499, length.out = 100)), 
               Eerror = rnorm(100, mean = 0, sd = 150), 
               Eprime = E + Eerror, 
               E_high = ifelse(E > 2200, 1, 0), 
               Eprime_high = ifelse(Eprime > 2200,1, 0),
               E_mis = ifelse(E_high != Eprime_high, 1, 0), 
               D = rbinom(n = 100, size = 1, p = plogis(-10 + 0.004*E))
               )

sp1 <- ggplot(pop0, aes(x = E, y = Eprime)) + geom_point() + scale_x_continuous(limits = c(1400, 2600), breaks = seq(1400,2600, 200)) + scale_y_continuous(limits = c(1200, 2800), breaks = seq(1200,2800, 200)) + theme_bw()

Figure1a <- sp1 + labs(title = "Cutpoint of 2,000", x = "True Value", y = "Measured Value") + geom_vline(xintercept = 2000) + geom_hline(yintercept = 2000, linetype = "dashed") 
# Figure1a

Figure1b <- sp1 + labs(title = "Original scenario (n = 100)", x = "True Value", y = "Measured Value") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")
# Figure1b
```

Want to create scatter plots for each distribution... need to create for each SD 
```{r scatterplots1}
# A. a = -4.5, b = 0.0019 
pop1 <- tibble(
  E = as.integer(runif(100, 1600, 2499)), 
  Eerror = rnorm(100, mean = 0, sd = 100), 
  Eprime = E + Eerror, 
  E_high = ifelse(E > 2200, 1, 0), 
  Eprime_high = ifelse(Eprime > 2200,1, 0), 
  E_mis = ifelse(E_high != Eprime_high, 1, 0),
  DA = as.factor(rbinom(n = 100, size = 1, p = plogis(-4.5 + 0.0019*E))),
  DB = as.factor(rbinom(n = 100, size = 1, p = plogis(-8.0 + 0.0035*E))),
  DC = as.factor(rbinom(n = 100, size = 1, p = plogis(-16.0 + 0.0072*E)))
  )

Scatter1A <- ggplot(pop1, aes(x = E, y = Eprime, color = DA)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "Original scenario (n = 100), RR = 1.66", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")


Scatter1B <- ggplot(pop1, aes(x = E, y = Eprime, color = DB)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "Original scenario (n = 100), RR = 2.49", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter1C <- ggplot(pop1, aes(x = E, y = Eprime, color = DC)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "Original scenario (n = 100), RR = 5.00", x = "True Value", y = "Measured Value ") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")
```


### Probability of misclassification and probability of disease
p probability of disease for each true exposure value, linear logistic model  ln(p/1-p) = a + b*E  
parameters a, b  
disease status 0 or 1 based on binomial n=1, prob of success = p from above  
Probability of misclassification is based on cumulative normal distribution. I based it on the difference between true value of E and the cutpoint; expected difference is 0.  


Figure 2 shows:  

1. expected probability of disease for a linear logistic model (a = -10, b = 0.004) AND  
2. probability of misclassification into low- and high- exposure based on normally distributed exposure measurement error (mean = 0, standard deviation = 150) 

Figure 3 shows: 

1. the expected probability of disease p as a quadratic function of the true exposure value E (p = 6.592 - 0.00673.E + 0.00000112E^2) over the specified range of values AND  
2. probability of misclassification (as in fig 2) 

```{r probD}
# expected probability of disease 
prob_D <- glm(D ~ E, data = pop0, family = binomial)
expected_pD <- predict(prob_D, pop0, type = "response")

# probability of misclassification
pop0$E_diff <- abs(pop0$E-2200)
pop0$a <- 1 - pnorm(pop0$E_diff, 0, 150)
# ggplot(pop, aes(x = E, y = a))+ geom_line() + geom_vline(xintercept = 2200) + theme_bw()

# Figure 2
ggplot(pop0, aes(x = E)) + geom_line(aes(y = expected_pD)) + geom_line(aes(y = a), linetype = 2) + geom_vline(xintercept = 2200) + theme_bw() + labs(title = "Fig 2. Prob of misclassification (dashed) & prob of disease (solid)", x = "True Value", y = "Probability")

# expected probability of disease p as a quadratic function of E
pop0$expected_pD_quad <- 6.592 - 0.00673*pop0$E + 0.00000172*(pop0$E**2)

# Figure 3:
ggplot(pop0, aes(x = E)) + geom_line(aes(y = expected_pD_quad)) + geom_line(aes(y = a), linetype = 2) + geom_vline(xintercept = 2200) + theme_bw() + labs(title = "Fig 3. Prob of misclassification (dashed) & prob of disease, quadratic term (solid)", x = "True Value", y = "Probability")

# probability of misclassification, varied SD to match simulation conditions
pop0$a100 <- 1 - pnorm(pop0$E_diff, 0, 100)
pop0$a300 <- 1 - pnorm(pop0$E_diff, 0, 300)
pop0$a500 <- 1 - pnorm(pop0$E_diff, 0, 500)
mis_plot <- ggplot(pop0, aes(x = E)) + geom_line(aes(y = expected_pD), color = "darkgrey") + geom_line(aes(y = a), linetype = 2) + geom_line(aes(y = a100), color = "green") + geom_line(aes(y = a300), color = "green3") + geom_line(aes(y = a500), color = "darkgreen") + geom_vline(xintercept = 2200) + theme_bw() + labs(title = "Prob of misclassification & prob of disease (solid)", x = "True Value", y = "Probability")

```
green lines are the expected probabilities of misclassification. 

Flegal et al. plot the probability of misclassification as a function of the distribution of the measurement error; not sure that it is actually the probability of misclassification? I think it may be more appropriate to simulate it and maybe re-create the population 100 times then get the proportion who were classified for each value of the exposure. Idk.


### Recreating simulations
Created a function to replicate the scenario from Flegal et al that outputs the Se and Sp overall and by disease status.  

Function arguments are:  

* N: sample size, uniformly distributed across range of exposure values  
* Emin, Emax: range of exposure values
* cutpt: the cutpoint for categorizing 
* a, b: parameters for the linear logistic probability of disease
* Nsigma: number of standard deviations of measurement error to go through 
* MEsigma: vector of the standard deviationsof measurement error  
* Nsims: number of simulations 

```{r flegalfunction}
Nsigma <- 3 # number of measurement error SD values - maybe this can come from MEsigma length?
MEsigma <- c(100, 300, 500) # values of measurement errors 

flegal_sim <- function(N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims){
  # initialize variables that we want to keep
  Se_all <- c()
  Se_D1 <- c()
  Se_D0 <- c()
  Sp_all <- c()
  Sp_D1 <- c()
  Sp_D0 <- c()
  RR_true <- c()
  RR_obs <- c()
  
    # fixed exposure values and disease status. Using seq to ensure same numbers every time
  E <- as.integer(seq(from = Emin, to = Emax, length.out = N))
  D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 
  
  for (j in 1:Nsigma){
    MEsig <- MEsigma[j]
  
    for (i in 1:Nsims){
      Eerror <- rnorm(N, mean = 0, sd = MEsig)
      Eprime <- E + Eerror
      E_high <- ifelse(E > cutpt, 1, 0)
      Eprime_high <- ifelse(Eprime > cutpt, 1, 0)
    
      Se_all[i] <- sum(Eprime_high==1 & E_high==1)/sum(E_high==1)
      Se_D1[i] <- sum(Eprime_high[D==1]==1 & E_high[D==1]==1)/sum(E_high[D==1]==1)
      Se_D0[i] <- sum(Eprime_high[D==0]==1 & E_high[D==0]==1)/sum(E_high[D==0]==1)
      
      Sp_all[i] <- sum(Eprime_high==0 & E_high==0)/sum(E_high==0)
      Sp_D1[i] <- sum(Eprime_high[D==1]==0 & E_high[D==1]==0)/sum(E_high[D==1]==0)
      Sp_D0[i] <- sum(Eprime_high[D==0]==0 & E_high[D==0]==0)/sum(E_high[D==0]==0)
      
      # based on equations from Flegal appendix. 
      RR_true[i] <- ((sum(D==1 & E_high==1)/sum(E_high==1))/(sum(D==1 & E_high==0)/sum(E_high==0)))
      RR_obs[i] <- ((sum(D==1 & Eprime_high==1)/sum(Eprime_high==1))/(sum(D==1 & Eprime_high==0)/sum(Eprime_high==0)))
      }
  params <- data.frame(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma, mean(Se_all), mean(Se_D1), mean(Se_D0), mean(Sp_all), mean(Sp_D1), mean(Sp_D0), mean(RR_true), mean(RR_obs))  
  colnames(params) <- c("N", "Emin", "Emax", "cutoff", "a", "b", "sims", "SD", "Se_all", "Se_D1", "Se_D0", "Sp_all", "Sp_D1", "Sp_D0", "RR_true", "RR_obs")
  return(params)
  }
}

```
Note about the RR calculations; they don't include whole population, but maybe that is similar to what one would measure irl? 

##### CASE 1  

So for the main simulations, they originally varied:  

* standard deviation of measurement error: 100, 300, 500  
* three sets of (a, b) parameters of linear logistic: (-4.5, 0.0019), (-8.0, 0.0035), (-16.0, 0.0072)  
and did 200 simulations of each on the nine combinations.  

```{r case1}
set.seed(303)
# function arguments: N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims
Nsigma <- 3
MEsigma <- c(100, 300, 500)
# A, B, C reflect the different regression models/ RR
case1A <- flegal_sim(100, 1600, 2499, 2200, -4.5, 0.0019, Nsigma, MEsigma, 200)
case1B <- flegal_sim(100, 1600, 2499, 2200, -8.0, 0.0035, Nsigma, MEsigma, 200)
case1C <- flegal_sim(100, 1600, 2499, 2200, -16.0, 0.0072, Nsigma, MEsigma, 200)

case1 <- rbind(case1A, case1B, case1C)
```


##### CASE 2  
Repeating the same but with 1000 simulations. Everything else is same. 
```{r, case2}
set.seed(303)
# function arguments: N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims
# A, B, C reflect the different regression models/ RR
case2A <- flegal_sim(100, 1600, 2499, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case2B <- flegal_sim(100, 1600, 2499, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case2C <- flegal_sim(100, 1600, 2499, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case2 <- rbind(case2A, case2B, case2C)
```
When we increase the simulations, the trends still hold. 


I think for N>900 (difference between min and max) it makes more sense to use uniform distribution to create the exposure. This means there may be variation between each run...
```{r unifunction}
Nsigma <- 3
MEsigma <- c(100, 300, 500) # values of measurement errors 
# Nsigma <- as.numeric(length(MEsigma)) # is this better? 

uni_sim <- function(N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims){
  # initialize variables that we want to keep
  Se_all <- c()
  Se_D1 <- c()
  Se_D0 <- c()
  Sp_all <- c()
  Sp_D1 <- c()
  Sp_D0 <- c()
  RR_true <- c()
  RR_obs <- c()
  
  E <- as.integer(runif(N, min = Emin, max = Emax)) #keeping this as int 
  D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 
  
  for (j in 1:Nsigma){
    MEsig <- MEsigma[j]
  
    for (i in 1:Nsims){
      Eerror <- rnorm(N, mean = 0, sd = MEsig)
      Eprime <- E + Eerror
      E_high <- ifelse(E > cutpt, 1, 0)
      Eprime_high <- ifelse(Eprime > cutpt, 1, 0)
    
      Se_all[i] <- sum(Eprime_high==1 & E_high==1)/sum(E_high==1)
      Se_D1[i] <- sum(Eprime_high[D==1]==1 & E_high[D==1]==1)/sum(E_high[D==1]==1)
      Se_D0[i] <- sum(Eprime_high[D==0]==1 & E_high[D==0]==1)/sum(E_high[D==0]==1)
      
      Sp_all[i] <- sum(Eprime_high==0 & E_high==0)/sum(E_high==0)
      Sp_D1[i] <- sum(Eprime_high[D==1]==0 & E_high[D==1]==0)/sum(E_high[D==1]==0)
      Sp_D0[i] <- sum(Eprime_high[D==0]==0 & E_high[D==0]==0)/sum(E_high[D==0]==0)
      
      # based on equations from Flegal appendix. 
      RR_true[i] <- ((sum(D==1 & E_high==1)/sum(E_high==1))/(sum(D==1 & E_high==0)/sum(E_high==0)))
      RR_obs[i] <- ((sum(D==1 & Eprime_high==1)/sum(Eprime_high==1))/(sum(D==1 & Eprime_high==0)/sum(Eprime_high==0)))
      }
  params <- data.frame(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma, mean(Se_all), mean(Se_D1), mean(Se_D0), mean(Sp_all), mean(Sp_D1), mean(Sp_D0), mean(RR_true), mean(RR_obs))  
  colnames(params) <- c("N", "Emin", "Emax", "cutoff", "a", "b", "sims", "SD", "Se_all", "Se_D1", "Se_D0", "Sp_all", "Sp_D1", "Sp_D0", "RR_true", "RR_obs")
  return(params)
  }
}

```
##### Uniform distribution with increased sample size
*Case 3*  
Increasing the number of samples from **100** to **1000*.  
From here on out, using 1,000 samples and 1,000 simulations.  
Comparing results from both functions
```{r case3}
set.seed(303)
# function arguments: N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims
# A, B, C reflect the different regression models/ RR
case3A <- flegal_sim(1000, 1600, 2499, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case3B <- flegal_sim(1000, 1600, 2499, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case3C <- flegal_sim(1000, 1600, 2499, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case3 <- rbind(case3A, case3B, case3C)

ucase3A <- uni_sim(1000, 1600, 2499, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
ucase3B <- uni_sim(1000, 1600, 2499, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
ucase3C <- uni_sim(1000, 1600, 2499, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

ucase3 <- rbind(ucase3A, ucase3B, ucase3C)

comp3 <- rbind(case3, ucase3)
```

*Case 4*  
Increasing the number of samples to 100,000. (1,000,000 took too long) Using unif distribution because of large N.     
```{r case4, eval = FALSE}
set.seed(303)
case4A <- uni_sim(100000, 1600, 2499, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case4B <- uni_sim(100000, 1600, 2499, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case4C <- uni_sim(100000, 1600, 2499, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case4 <- rbind(case4A, case4B, case4C)
```


What happens if we make exposure normally distributed?  

##### CASE 5  
New function for normal distribution
```{r createnorm}
normal_sim <- function(N, Emean, Esigma, cutoff, a, b, Nsigma, MEsigma, Nsims){
# initialize variables that we want to keep
  Se_all <- c()
  Se_D1 <- c()
  Se_D0 <- c()
  Sp_all <- c()
  Sp_D1 <- c()
  Sp_D0 <- c()
  RR_true <- c()
  RR_obs <- c()

  # have to 'fix' the true exposure value in order to fix the disease status. 
  E <- rnorm(N, mean = Emean, sd = Esigma)
  D <- rbinom(n = N, size = 1, p = plogis(a + b*E))
for (j in 1:Nsigma){
  for (i in 1:N){
    Eerror <- rnorm(N, mean = 0, sd = MEsigma)
    Eprime <- E + Eerror
    E_high <- ifelse(E > cutoff, 1, 0)
    Eprime_high <- ifelse(Eprime > cutoff,1, 0)

    Se_all[i] <- sum(Eprime_high==1 & E_high==1)/sum(E_high==1)
    Se_D1[i] <- sum(Eprime_high[D==1]==1 & E_high[D==1]==1)/sum(E_high[D==1]==1)
    Se_D0[i] <- sum(Eprime_high[D==0]==1 & E_high[D==0]==1)/sum(E_high[D==0]==1)
  
    Sp_all[i] <- sum(Eprime_high==0 & E_high==0)/sum(E_high==0)
    Sp_D1[i] <- sum(Eprime_high[D==1]==0 & E_high[D==1]==0)/sum(E_high[D==1]==0)
    Sp_D0[i] <- sum(Eprime_high[D==0]==0 & E_high[D==0]==0)/sum(E_high[D==0]==0)
  
    RR_true[i] <- (sum(D==1 & E_high==1)/sum(E_high==1))/(sum(D==1 & E_high==0)/sum(E_high==0))
      RR_obs[i] <- ((sum(D==1 & Eprime_high==1)/sum(Eprime_high==1))/(sum(D==1 & Eprime_high==0)/sum(Eprime_high==0)))
        }
    params <- data.frame(N, Emean, Esigma, cutoff, a, b, Nsims, MEsigma, mean(Se_all), mean(Se_D1), mean(Se_D0), mean(Sp_all), mean(Sp_D1), mean(Sp_D0), mean(RR_true), mean(RR_obs))
    colnames(params) <- c("N", "Emean", "ESD", "cutoff", "a", "b", "sims", "SD", "Se_all", "Se_D1", "Se_D0", "Sp_all", "Sp_D1", "Sp_D0", "RR_true", "RR_obs")
  }
  return(params)
}
```


Previous range is 1,600 to 2,499. 
If mean = midpoint of 2050, SD of 150 would make most of the values fall within 1,600 - 2,5000 range. 
Is there reason to believe that the exposure distribution would be affected by probability of disease?   


Case 5: Changing the exposure distribution to ~N(2050, 150)    
```{r case5}
set.seed(303)
Nsigma <- 3
MEsigma <- c(100, 300, 500)
# A. a = -4.5, b = 0.0019 
# B. a = -8.0, b = 0.0035
# C. a = -16.0, b = 0.0072

case5A <- normal_sim(1000, 2050, 150, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case5B <- normal_sim(1000, 2050, 150, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case5C <- normal_sim(1000, 2050, 150, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case5 <- rbind(case5A, case5B, case5C)
```
Seems like there is still have some level of differential misclassification, but smaller difference between diseased and non-diseased for both Se and Sp.  


##### Other ~N
Case 6: Normal distribution around the cutpoint (mean 2,200)  
Case 7: exposure ~N but 'high' is rare (mean 1,900)

```{r case6sim}
# A. a = -4.5, b = 0.0019 
# B. a = -8.0, b = 0.0035
# C. a = -16.0, b = 0.0072
set.seed(303)
Nsigma <- 3
MEsigma <- c(100, 300, 500)

case6A <- normal_sim(1000, 2050, 150, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case6B <- normal_sim(1000, 2050, 150, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case6C <- normal_sim(1000, 2050, 150, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case6 <- rbind(case6A, case6B, case6C)

case7A <- normal_sim(1000, 2050, 150, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case7B <- normal_sim(1000, 2050, 150, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case7C <- normal_sim(1000, 2050, 150, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case7 <- rbind(case7A, case7B, case7C)

case8A <- normal_sim(1000, 2050, 150, 2200, -4.5, 0.0019, Nsigma, MEsigma, 1000)
case8B <- normal_sim(1000, 2050, 150, 2200, -8.0, 0.0035, Nsigma, MEsigma, 1000)
case8C <- normal_sim(1000, 2050, 150, 2200, -16.0, 0.0072, Nsigma, MEsigma, 1000)

case8 <- rbind(case8A, case8B, case8C)
```

at this point, I typically export as csv but with uniform cases in one grouping and normal separately, since the variables retained are slightly different. I then create an xlsx with one tab for each then one where they are combined....
```{r savestuff}
# case4 is 100,000
unifs <- rbind(case1, case2, case3, case4)
norms <- rbind(case5, case6, case7, case8)
write.csv(unifs, "uniform.csv")
write.csv(norms, "normal.csv")
```


For now I am creating one plot at a time. It is probably possible to do this more efficiently (maybe long format vs. wide) but that is a problem for a later time...
```{r scatterplots2}
# A. a = -4.5, b = 0.0019 
# B. a = -8.0, b = 0.0035
# 3. a = -16.0, b = 0.0072
pop5 <- tibble(
  E = rnorm(1000, mean = 2050, sd = 150), 
  Eerror100 = rnorm(1000, mean = 0, sd = 100), Eprime100 = E + Eerror100, 
  Eerror300 = rnorm(1000, mean = 0, sd = 300), Eprime300 = E + Eerror300, 
  Eerror500 = rnorm(1000, mean = 0, sd = 500), Eprime500 = E + Eerror500, 
  E_high = ifelse(E > 2200, 1, 0), Eprime100_high = ifelse(Eprime100 > 2200, 1, 0), 
  Eprime300_high = ifelse(Eprime300 > 2200, 1, 0), Eprime500_high = ifelse(Eprime500 > 2200, 1, 0), 
  DA = as.factor(rbinom(n = 1000, size = 1, p = plogis(-4.5 + 0.0019*E))),
  DB = as.factor(rbinom(n = 1000, size = 1, p = plogis(-8.0 + 0.0035*E))),
  DC = as.factor(rbinom(n = 1000, size = 1, p = plogis(-16.0 + 0.0072*E)))
  )

Scatter5A_100 <- ggplot(pop5, aes(x = E, y = Eprime100, color = DA)) + geom_point(alpha = 0.5) + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "E ~N(2050,150), ME ~N(0,100) RR = 1.66", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed") + theme_classic()

Scatter5B_100 <- ggplot(pop5, aes(x = E, y = Eprime100, color = DB)) + geom_point(alpha = 0.5) + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "E ~N(2050,150), ME ~N(0,100) RR = 2.49", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed") + theme_classic()

Scatter5C_100 <- ggplot(pop5, aes(x = E, y = Eprime100, color = DC)) + geom_point(alpha = 0.5) + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "E ~N(2050,150), ME ~N(0,100) RR = 5.00", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed") + theme_classic()
```


Which RR and SD combos to plot? 
```{r morescatter}
# case 6: E <- rnorm(n, mean = 2200, sd = 150)
pop6 <- data.frame(
  E = rnorm(1000, mean = 2200, sd = 150), 
  Eerror = rnorm(1000, mean = 0, sd = 100), 
  Eprime = E + Eerror, 
  E_high = ifelse(E > 2200, 1, 0), 
  Eprime_high = ifelse(Eprime > 2200, 1, 0), 
  E_mis = ifelse(E_high != Eprime_high, 1, 0),
  DA = as.factor(rbinom(n = 1000, size = 1, p = plogis(-4.5 + 0.0019*E))),
  DB = as.factor(rbinom(n = 1000, size = 1, p = plogis(-8.0 + 0.0035*E))),
  DC = as.factor(rbinom(n = 1000, size = 1, p = plogis(-16.0 + 0.0072*E)))
  )

Scatter6A <- ggplot(pop_6, aes(x = E, y = Eprime, color = DA)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "E ~Normal(2200, 150), RR = 1.66", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter6B <- ggplot(pop_6, aes(x = E, y = Eprime, color = DB)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "E ~Normal(2200, 150), RR = 2.49", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter6C <- ggplot(pop_6, aes(x = E, y = Eprime, color = DC)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "E ~Normal(2200, 150), RR = 5.00", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")


# case7: E ~N(mean = 1900, sd = 150) rare exposure
pop7 <- data.frame(
  E = rnorm(1000, mean = 1900, sd = 150), 
  Eerror = rnorm(1000, mean = 0, sd = 100), 
  Eprime = E + Eerror, 
  E_high = ifelse(E > 2200, 1, 0), 
  Eprime_high = ifelse(Eprime > 2200, 1, 0), 
  E_mis = ifelse(E_high != Eprime_high, 1, 0),
  DA = as.factor(rbinom(n = 1000, size = 1, p = plogis(-4.5 + 0.0019*E))),
  DB = as.factor(rbinom(n = 1000, size = 1, p = plogis(-8.0 + 0.0035*E))),
  DC = as.factor(rbinom(n = 1000, size = 1, p = plogis(-16.0 + 0.0072*E)))
  )

Scatter7A <- ggplot(pop_7, aes(x = E, y = Eprime, color = DA)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "E ~Normal(1900, 150), RR = 1.66", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter7B <- ggplot(pop_7, aes(x = E, y = Eprime, color = DB)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "E ~Normal(1900, 150), RR = 2.49", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter7C <- ggplot(pop_7, aes(x = E, y = Eprime, color = DC)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "E ~Normal(1900, 150), RR = 5.00", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")


# uniform but n = 1000
pop8 <- data.frame(
  E = runif(1000, min = 1600, max = 2499),
  Eerror = rnorm(1000, mean = 0, sd = 100), 
  Eprime = E + Eerror, 
  E_high = ifelse(E > 2200, 1, 0), 
  Eprime_high = ifelse(Eprime > 2200, 1, 0), 
  E_mis = ifelse(E_high != Eprime_high, 1, 0),
  DA = as.factor(rbinom(n = 1000, size = 1, p = plogis(-4.5 + 0.0019*E))),
  DB = as.factor(rbinom(n = 1000, size = 1, p = plogis(-8.0 + 0.0035*E))),
  DC = as.factor(rbinom(n = 1000, size = 1, p = plogis(-16.0 + 0.0072*E)))
  )

Scatter8A <- ggplot(pop_8, aes(x = E, y = Eprime, color = DA)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400,2800, 200)) + theme_bw() + labs(title = "Uniform with n = 1,000, RR = 1.66", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter8B <- ggplot(pop_8, aes(x = E, y = Eprime, color = DB)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "Uniform with n = 1,000, RR = 2.49", x = "True Value", y = "Measured Value", color = "Disease") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")

Scatter8C <- ggplot(pop_8, aes(x = E, y = Eprime, color = DC)) + geom_point() + scale_x_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + scale_y_continuous(limits = c(1400, 2800), breaks = seq(1400, 2800, 200)) + theme_bw() + labs(title = "Uniform with n = 1,000, RR = 5.00", x = "True Value", y = "Measured Value", color = "Disease" ) + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")
```
