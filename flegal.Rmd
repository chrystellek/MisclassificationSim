---
title: "Flegal Diff Misclassification From Non-diff Measurement"
author: "Chrystelle Kiang"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
```


The first goal is to re-create Flegal et al's simulation in "Differential Misclassification Arising from Nondifferential Errors in Exposure Measurement"  

E true exposure values, 100 samples evenly spaced over specified interval  
E' measured exposure values: true + error term, where Error terms ~N with mean 0, "specified" SD  
true exposure values range integers 1,600 - 2,499, high/low cutoff of 2,200  
standard deviation of measurement error varied at 100, 300, 500  

### Recreating simulations
Created a function to replicate the scenario from Flegal et al that outputs the Se and Sp overall and by disease status.
The first time I made this function, disease was being assigned anew each simulation except for exposure. (Is in a chunk later in code). To reduce variability, created function that will fix the exposure AND disease status before measurement error is simulated. Note though that Flegal et al. did not fix disease before simulations, since they present averages of the "true" and "observed" RRs (Table 3).  


# Fixed disease before simulations
Function arguments:
* N: sample size, uniformly distributed across range of exposure values  
* Emin, Emax: range of exposure values
* cutpt: the cutpoint for categorizing 
* a, b: parameters for the linear logistic probability of disease
* MEsigma: vector of the standard deviations of measurement error  
* Nsims: number of simulations 

```{r flegalfunction}
flegal_sim_fixed <- function(N, Emin, Emax, cutpt, a, b, MEsigma, Nsims){
  # fixed exposure values and disease status. Using seq to ensure same numbers as paper
  E <- as.integer(seq(from = Emin, to = Emax, length.out = N))
  D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 

  values <- matrix(nrow = Nsims, ncol = 8, dimnames = list(c(), c("Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))
  results <- matrix(nrow = length(MEsigma), ncol = 16, dimnames = list(c(),c("N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "SeAll", "SeD1", "SeD0", "SpAll", "SpD1", "SpD0", "RRtrue", "RRobs")))
  
   allresults <- data.frame()
 
    
  for (j in seq_along(MEsigma)){
    
    for (i in 1:Nsims){
      Eerror <- rnorm(N, mean = 0, sd = MEsigma)
      Eprime <- E + Eerror
      Ehigh <- ifelse(E > cutpt, 1, 0)
      Eprimehigh <- ifelse(Eprime > cutpt, 1, 0)
    
      values[i,1] <- sum(Eprimehigh==1 & Ehigh==1)/sum(Ehigh==1)
      values[i,2] <- sum(Eprimehigh[D==1]==1 & Ehigh[D==1]==1)/sum(Ehigh[D==1]==1)
      values[i,3] <- sum(Eprimehigh[D==0]==1 & Ehigh[D==0]==1)/sum(Ehigh[D==0]==1)
      
      values[i,4] <- sum(Eprimehigh==0 & Ehigh==0)/sum(Ehigh==0)
      values[i,5] <- sum(Eprimehigh[D==1]==0 & Ehigh[D==1]==0)/sum(Ehigh[D==1]==0)
      values[i,6] <- sum(Eprimehigh[D==0]==0 & Ehigh[D==0]==0)/sum(Ehigh[D==0]==0)
      
      values[i,7] <- (sum(D==1 & Ehigh==1)/sum(Ehigh==1))/(sum(D==1 & Ehigh==0)/sum(Ehigh==0))
      values[i,8] <- (sum(D==1 & Eprimehigh==1)/sum(Eprimehigh==1))/(sum(D==1 & Eprimehigh==0)/sum(Eprimehigh==0))
     }
    # "N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "SeAll", "SeD1", "SeD0", "SpAll", "SpD1", "SpD0", "RRtrue", "RRobs"
  results[j,] <- cbind(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma[[j]], mean(values[,1]), mean(values[,2]), mean(values[,3]), mean(values[,4]), mean(values[,5]), mean(values[,6]), mean(values[,7]), mean(values[,8]))
  
  }
 
  allresults <- as.data.frame(results)
  return(allresults)
}

orange <- c(100, 300, 500)
test1 <- flegal_sim_fixed(100, 1600, 2499, 2200, -4.5, 0.0019, orange, 200)
```
Okay... I am a little concerned about how close the values are together. I have tried this approach outside of the function and it seemed to work. So... what does that mean now about the main point now that we removed variability in disease assignment?



##### CASE 1  

So for the main simulations, they originally varied:  

* standard deviation of measurement error: 100, 300, 500  
* three sets of (a, b) parameters for disease: (-4.5, 0.0019), (-8.0, 0.0035), (-16.0, 0.0072)  
and did 200 simulations of each on the nine combinations.  

```{r case1}
set.seed(303)
# function arguments: N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims
MEsigma <- c(100, 300, 500)
# A, B, C reflect the different regression models/ RR
case1A <- flegal_sim_fixed(100, 1600, 2499, 2200, -4.5, 0.0019, MEsigma, 200)
case1B <- flegal_sim_fixed(100, 1600, 2499, 2200, -8.0, 0.0035, MEsigma, 200)
case1C <- flegal_sim_fixed(100, 1600, 2499, 2200, -16.0, 0.0072, MEsigma, 200)

case1 <- rbind(case1A, case1B, case1C)
case1
```
The one-off results differ from the original across the board  



##### CASE 2  
Repeating the same but with 1000 simulations. Everything else is same. 
```{r, case2}
set.seed(303)
# function arguments: N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims
# A, B, C reflect the different regression models/ RR
case2A <- flegal_sim_fixed(100, 1600, 2499, 2200, -4.5, 0.0019, MEsigma, 1000)
case2B <- flegal_sim_fixed(100, 1600, 2499, 2200, -8.0, 0.0035, MEsigma, 1000)
case2C <- flegal_sim_fixed(100, 1600, 2499, 2200, -16.0, 0.0072, MEsigma, 1000)

case2 <- rbind(case2A, case2B, case2C)

case2
```


I think for N>900 (difference between min and max) it makes more sense to use uniform distribution to create the exposure. This means there may be variation in exposure values in each run... 
```{r unifunction}
uni_sim_fixed <- function(N, Emin, Emax, cutpt, a, b, MEsigma, Nsims){
    
  E <- as.integer(runif(N, min = Emin, max = Emax))  
  D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 
  
  values <- matrix(nrow = Nsims, ncol = 8, dimnames = list(c(), c("Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))
  results <- matrix(nrow = length(MEsigma), ncol = 16, dimnames = list(c(),c("N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "SeAll", "SeD1", "SeD0", "SpAll", "SpD1", "SpD0", "RRtrue", "RRobs")))

  
  for (j in seq_along(MEsigma)){
  
    for (i in 1:Nsims){
      Eerror <- rnorm(N, mean = 0, sd = MEsigma)
      Eprime <- E + Eerror
      Ehigh <- ifelse(E > cutpt, 1, 0)
      Eprimehigh <- ifelse(Eprime > cutpt, 1, 0)
    
      values[i,1] <- sum(Eprimehigh==1 & Ehigh==1)/sum(Ehigh==1)
      values[i,2] <- sum(Eprimehigh[D==1]==1 & Ehigh[D==1]==1)/sum(Ehigh[D==1]==1)
      values[i,3] <- sum(Eprimehigh[D==0]==1 & Ehigh[D==0]==1)/sum(Ehigh[D==0]==1)
      
      values[i,4] <- sum(Eprimehigh==0 & Ehigh==0)/sum(Ehigh==0)
      values[i,5] <- sum(Eprimehigh[D==1]==0 & Ehigh[D==1]==0)/sum(Ehigh[D==1]==0)
      values[i,6] <- sum(Eprimehigh[D==0]==0 & Ehigh[D==0]==0)/sum(Ehigh[D==0]==0)
      
      values[i,7] <- (sum(D==1 & Ehigh==1)/sum(Ehigh==1))/(sum(D==1 & Ehigh==0)/sum(Ehigh==0))
      values[i,8] <- (sum(D==1 & Eprimehigh==1)/sum(Eprimehigh==1))/(sum(D==1 & Eprimehigh==0)/sum(Eprimehigh==0))
 
    }
  #  "N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs"
  results[j,] <- cbind(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma[[j]], mean(values[,1]), mean(values[,2]), mean(values[,3]), mean(values[,4]), mean(values[,5]), mean(values[,6]), mean(values[,7]), mean(values[,8]))
  }
  
  results <- as.data.frame(results)
  return(results)
  
}

```
##### Uniform distribution with increased sample size
*Case 3*  
Increasing the number of samples from **100** to **1000*.  
From here on out, using 1,000 samples and 1,000 simulations.  
Comparing results from both functions
```{r case3}
set.seed(303)
# function arguments: N, Emin, Emax, cutpt, a, b, Nsigma, MEsigma, Nsims
# A, B, C reflect the different regression models/ RR
case3A <- flegal_sim_fixed(1000, 1600, 2499, 2200, -4.5, 0.0019, MEsigma, 1000)
case3B <- flegal_sim_fixed(1000, 1600, 2499, 2200, -8.0, 0.0035, MEsigma, 1000)
case3C <- flegal_sim_fixed(1000, 1600, 2499, 2200, -16.0, 0.0072, MEsigma, 1000)

scase3 <- rbind(case3A, case3B, case3C)

ucase3A <- uni_sim_fixed(1000, 1600, 2499, 2200, -4.5, 0.0019, MEsigma, 1000)
ucase3B <- uni_sim_fixed(1000, 1600, 2499, 2200, -8.0, 0.0035, MEsigma, 1000)
ucase3C <- uni_sim_fixed(1000, 1600, 2499, 2200, -16.0, 0.0072, MEsigma, 1000)

ucase3 <- rbind(ucase3A, ucase3B, ucase3C)

comp3 <- rbind(scase3, ucase3) #comparing how the seq vs. uniform functions do 

case3 <- ucase3
```

*Case 4*  
Increasing the number of samples to 100,000 (1,000,000 took too long)      
```{r case4, eval = F}
set.seed(303)
case4A <- uni_sim_fixed(100000, 1600, 2499, 2200, -4.5, 0.0019, MEsigma, 1000)
case4B <- uni_sim_fixed(100000, 1600, 2499, 2200, -8.0, 0.0035, MEsigma, 1000)
case4C <- uni_sim_fixed(100000, 1600, 2499, 2200, -16.0, 0.0072, MEsigma, 1000)

case4 <- rbind(case4A, case4B, case4C)
case4
```


What happens if we make exposure normally distributed?  

##### CASE 5  
New function for normal distribution
```{r createnorm}
normal_sim_fixed <- function(N, Emean, Esigma, cutpt, a, b, MEsigma, Nsims){
  
  E <- rnorm(N, mean = Emean, sd = Esigma)
  D <- rbinom(n = N, size = 1, p = plogis(a + b*E))
  
  values <- matrix(nrow = Nsims, ncol = 8, dimnames = list(c(), c("Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))
  params <- matrix(nrow = length(MEsigma), ncol = 16, dimnames = list(c(),c("N", "Emean", "ESD", "cutoff", "a", "b", "Nsims", "ME", "Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))

  
for (j in seq_along(MEsigma)){
  
  for (i in 1:Nsims){
      Eerror <- rnorm(N, mean = 0, sd = MEsigma)
      Eprime <- E + Eerror
      Ehigh <- ifelse(E > cutpt, 1, 0)
      Eprimehigh <- ifelse(Eprime > cutpt, 1, 0)
    
      values[i,1] <- sum(Eprimehigh==1 & Ehigh==1)/sum(Ehigh==1)
      values[i,2] <- sum(Eprimehigh[D==1]==1 & Ehigh[D==1]==1)/sum(Ehigh[D==1]==1)
      values[i,3] <- sum(Eprimehigh[D==0]==1 & Ehigh[D==0]==1)/sum(Ehigh[D==0]==1)
      
      values[i,4] <- sum(Eprimehigh==0 & Ehigh==0)/sum(Ehigh==0)
      values[i,5] <- sum(Eprimehigh[D==1]==0 & Ehigh[D==1]==0)/sum(Ehigh[D==1]==0)
      values[i,6] <- sum(Eprimehigh[D==0]==0 & Ehigh[D==0]==0)/sum(Ehigh[D==0]==0)
      
      values[i,7] <- (sum(D==1 & Ehigh==1)/sum(Ehigh==1))/(sum(D==1 & Ehigh==0)/sum(Ehigh==0))
      values[i,8] <- (sum(D==1 & Eprimehigh==1)/sum(Eprimehigh==1))/(sum(D==1 & Eprimehigh==0)/sum(Eprimehigh==0))
  }
#  "N", "Emean", "ESD", "cutoff", "a", "b", "Nsims", "ME", "Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs"
    params[j,] <- cbind(N, Emean, Esigma, cutpt, a, b, Nsims, MEsigma[[j]], mean(values[,1]), mean(values[,2]), mean(values[,3]), mean(values[,4]), mean(values[,5]), mean(values[,6]), mean(values[,7]), mean(values[,8]))

}
  
  params <- as.data.frame(params) 
  return(params)
}

```

Previous range is 1,600 to 2,499. 
If mean = midpoint of 2050, SD of 150 would make most of the values fall within 1,600 - 2,5000 range. 
Is there reason to believe that the exposure distribution would be affected by probability of disease?   


Case 5: Changing the exposure distribution to ~N(2050, 150)    
```{r case5}
set.seed(303)
# A. a = -4.5, b = 0.0019; B. a = -8.0, b = 0.0035; C. a = -16.0, b = 0.0072
# N, Emean, Esigma, cutpt, a, b, MEsigma, Nsims
case5A <- normal_sim_fixed(1000, 2050, 150, 2200, -4.5, 0.0019, MEsigma, 1000)
case5B <- normal_sim_fixed(1000, 2050, 150, 2200, -8.0, 0.0035, MEsigma, 1000)
case5C <- normal_sim_fixed(1000, 2050, 150, 2200, -16.0, 0.0072, MEsigma, 1000)

case5 <- rbind(case5A, case5B, case5C)

##### Other ~N
# Case 6: Normal distribution around the cutpoint (mean 2,200)  
# Case 7: exposure ~N but 'high' is rare (mean 1,900)
# Case 8: big sample size

case6A <- normal_sim_fixed(1000, 2200, 150, 2200, -4.5, 0.0019, MEsigma, 1000)
case6B <- normal_sim_fixed(1000, 2200, 150, 2200, -8.0, 0.0035, MEsigma, 1000)
case6C <- normal_sim_fixed(1000, 2200, 150, 2200, -16.0, 0.0072, MEsigma, 1000)

case6 <- rbind(case6A, case6B, case6C)

case7A <- normal_sim_fixed(1000, 1900, 150, 2200, -4.5, 0.0019, MEsigma, 1000)
case7B <- normal_sim_fixed(1000, 1900, 150, 2200, -8.0, 0.0035, MEsigma, 1000)
case7C <- normal_sim_fixed(1000, 1900, 150, 2200, -16.0, 0.0072, MEsigma, 1000)

case7 <- rbind(case7A, case7B, case7C)
```


```{r huge, eval = FALSE}
set.seed(303)
case8A <- normal_sim_fixed(100000, 2050, 150, 2200, -4.5, 0.0019, MEsigma, 1000)
case8B <- normal_sim_fixed(100000, 2050, 150, 2200, -8.0, 0.0035, MEsigma, 1000)
case8C <- normal_sim_fixed(100000, 2050, 150, 2200, -16.0, 0.0072, MEsigma, 1000)

case8 <- rbind(case8A, case8B, case8C)
```


```{r savestuff, include = FALSE, eval = FALSE}
# at this point, I typically export as csv but with uniform cases in one grouping and normal separately, since the variables retained are slightly different. I then create an xlsx to combine
unifs <- rbind(case1, case2, case3, case4)
norms <- rbind(case5, case6, case7, case8)
write.csv(unifs, "uniform.csv")
write.csv(norms, "normal.csv")
```

#### Disease not fixed at each simulation

```{r morefunctions, eval = FALSE}
uni_sim <- function(N, Emin, Emax, cutpt, a, b, MEsigma, Nsims){
    
  E <- as.integer(runif(N, min = Emin, max = Emax))  
  
  values <- matrix(nrow = Nsims, ncol = 8, dimnames = list(c(), c("Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))
  results <- matrix(nrow = length(MEsigma), ncol = 16, dimnames = list(c(),c("N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "SeAll", "SeD1", "SeD0", "SpAll", "SpD1", "SpD0", "RRtrue", "RRobs")))

  
  for (j in seq_along(MEsigma)){
  
    for (i in 1:Nsims){
      D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 
      Eerror <- rnorm(N, mean = 0, sd = MEsigma)
      Eprime <- E + Eerror
      Ehigh <- ifelse(E > cutpt, 1, 0)
      Eprimehigh <- ifelse(Eprime > cutpt, 1, 0)
    
      values[i,1] <- sum(Eprimehigh==1 & Ehigh==1)/sum(Ehigh==1)
      values[i,2] <- sum(Eprimehigh[D==1]==1 & Ehigh[D==1]==1)/sum(Ehigh[D==1]==1)
      values[i,3] <- sum(Eprimehigh[D==0]==1 & Ehigh[D==0]==1)/sum(Ehigh[D==0]==1)
      
      values[i,4] <- sum(Eprimehigh==0 & Ehigh==0)/sum(Ehigh==0)
      values[i,5] <- sum(Eprimehigh[D==1]==0 & Ehigh[D==1]==0)/sum(Ehigh[D==1]==0)
      values[i,6] <- sum(Eprimehigh[D==0]==0 & Ehigh[D==0]==0)/sum(Ehigh[D==0]==0)
      
      values[i,7] <- (sum(D==1 & Ehigh==1)/sum(Ehigh==1))/(sum(D==1 & Ehigh==0)/sum(Ehigh==0))
      values[i,8] <- (sum(D==1 & Eprimehigh==1)/sum(Eprimehigh==1))/(sum(D==1 & Eprimehigh==0)/sum(Eprimehigh==0))
 
    }
  #  "N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs"
  results[j,] <- cbind(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma[[j]], mean(values[,1]), mean(values[,2]), mean(values[,3]), mean(values[,4]), mean(values[,5]), mean(values[,6]), mean(values[,7]), mean(values[,8]))
  }
  
  results <- as.data.frame(results)
  return(results)
  
}

# Normal
normal_sim <- function(N, Emean, Esigma, cutpt, a, b, MEsigma, Nsims){
  E <- rnorm(N, mean = Emean, sd = Esigma)
  
  values <- matrix(nrow = Nsims, ncol = 8, dimnames = list(c(), c("Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))
  
  params <- matrix(nrow = length(MEsigma), ncol = 16, dimnames = list(c(),c("N", "Emean", "ESD", "cutoff", "a", "b", "Nsims", "ME", "Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))

for (j in seq_along(MEsigma)){
  
  for (i in 1:Nsims){
      D <- rbinom(n = N, size = 1, p = plogis(a + b*E))
      Eerror <- rnorm(N, mean = 0, sd = MEsigma)
      Eprime <- E + Eerror
      Ehigh <- ifelse(E > cutpt, 1, 0)
      Eprimehigh <- ifelse(Eprime > cutpt, 1, 0)
    
      values[i,1] <- sum(Eprimehigh==1 & Ehigh==1)/sum(Ehigh==1)
      values[i,2] <- sum(Eprimehigh[D==1]==1 & Ehigh[D==1]==1)/sum(Ehigh[D==1]==1)
      values[i,3] <- sum(Eprimehigh[D==0]==1 & Ehigh[D==0]==1)/sum(Ehigh[D==0]==1)
      
      values[i,4] <- sum(Eprimehigh==0 & Ehigh==0)/sum(Ehigh==0)
      values[i,5] <- sum(Eprimehigh[D==1]==0 & Ehigh[D==1]==0)/sum(Ehigh[D==1]==0)
      values[i,6] <- sum(Eprimehigh[D==0]==0 & Ehigh[D==0]==0)/sum(Ehigh[D==0]==0)
      
      values[i,7] <- (sum(D==1 & Ehigh==1)/sum(Ehigh==1))/(sum(D==1 & Ehigh==0)/sum(Ehigh==0))
      values[i,8] <- (sum(D==1 & Eprimehigh==1)/sum(Eprimehigh==1))/(sum(D==1 & Eprimehigh==0)/sum(Eprimehigh==0))
  }
  
    params[j,] <- cbind(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma[[j]], mean(values[,1]), mean(values[,2]), mean(values[,3]), mean(values[,4]), mean(values[,5]), mean(values[,6]), mean(values[,7]), mean(values[,8]))

}
  
  params <- as.data.frame(params) 
  return(params)
}

flegal_sim <- function(N, Emin, Emax, cutpt, a, b, MEsigma, Nsims){
 
   E <- as.integer(seq(from = Emin, to = Emax, length.out = N))
  values <- matrix(nrow = Nsims, ncol = 8, dimnames = list(c(), c("Se", "SeD1", "SeD0", "Sp", "SpD1", "SpD0", "RRtrue", "RRobs")))
  results <- matrix(nrow = length(MEsigma), ncol = 16, dimnames = list(c(),c("N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "SeAll", "SeD1", "SeD0", "SpAll", "SpD1", "SpD0", "RRtrue", "RRobs")))
  
   allresults <- data.frame()
 
    
  for (j in seq_along(MEsigma)){
    
    for (i in 1:Nsims){
      D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 
      Eerror <- rnorm(N, mean = 0, sd = MEsigma)
      Eprime <- E + Eerror
      Ehigh <- ifelse(E > cutpt, 1, 0)
      Eprimehigh <- ifelse(Eprime > cutpt, 1, 0)
    
      values[i,1] <- sum(Eprimehigh==1 & Ehigh==1)/sum(Ehigh==1)
      values[i,2] <- sum(Eprimehigh[D==1]==1 & Ehigh[D==1]==1)/sum(Ehigh[D==1]==1)
      values[i,3] <- sum(Eprimehigh[D==0]==1 & Ehigh[D==0]==1)/sum(Ehigh[D==0]==1)
      
      values[i,4] <- sum(Eprimehigh==0 & Ehigh==0)/sum(Ehigh==0)
      values[i,5] <- sum(Eprimehigh[D==1]==0 & Ehigh[D==1]==0)/sum(Ehigh[D==1]==0)
      values[i,6] <- sum(Eprimehigh[D==0]==0 & Ehigh[D==0]==0)/sum(Ehigh[D==0]==0)
      
      values[i,7] <- (sum(D==1 & Ehigh==1)/sum(Ehigh==1))/(sum(D==1 & Ehigh==0)/sum(Ehigh==0))
      values[i,8] <- (sum(D==1 & Eprimehigh==1)/sum(Eprimehigh==1))/(sum(D==1 & Eprimehigh==0)/sum(Eprimehigh==0))
     }
    # "N", "Emin", "Emax", "cutoff", "a", "b", "Nsims", "ME", "SeAll", "SeD1", "SeD0", "SpAll", "SpD1", "SpD0", "RRtrue", "RRobs"
  results[j,] <- cbind(N, Emin, Emax, cutpt, a, b, Nsims, MEsigma[[j]], mean(values[,1]), mean(values[,2]), mean(values[,3]), mean(values[,4]), mean(values[,5]), mean(values[,6]), mean(values[,7]), mean(values[,8]))
  
  }
 
  allresults <- as.data.frame(results)
  return(allresults)
}
```

  
This is the first working one that does not have disease fixed across simulations- used to create results shared Aug 10 with group
```{r function1, eval = FALSE}
flegal_sim <- function(N, min, max, cutpt, a, b, sigma, sims){
# True exposure
E <- seq(from = min, to = max, length.out = N) 
  # initialize variables that we want to keep
  Se_all <- c()
  Se_D1 <- c()
  Se_D0 <- c()
  Sp_all <- c()
  Sp_D1 <- c()
  Sp_D0 <- c()
  RR_true <- c()
  RR_obs <- c()
  
  for (i in 1:sims){
  # set up
    Eerror <- rnorm(N, mean = 0, sd = sigma)
    Eprime <- E + Eerror
    E_high <- ifelse(E > cutpt, 1, 0)
    Eprime_high <- ifelse(Eprime > cutpt,1, 0)
    D <- rbinom(n = N, size = 1, p = plogis(a + b*E)) 
    
  # storing Se, Sp, RR values for each simulation
    Se_all[i] <- sum(Eprime_high==1 & E_high==1)/sum(E_high==1)
    Se_D1[i] <- sum(Eprime_high[D==1]==1 & E_high[D==1]==1)/sum(E_high[D==1]==1)
    Se_D0[i] <- sum(Eprime_high[D==0]==1 & E_high[D==0]==1)/sum(E_high[D==0]==1)
    
    Sp_all[i] <- sum(Eprime_high==0 & E_high==0)/sum(E_high==0)
    Sp_D1[i] <- sum(Eprime_high[D==1]==0 & E_high[D==1]==0)/sum(E_high[D==1]==0)
    Sp_D0[i] <- sum(Eprime_high[D==0]==0 & E_high[D==0]==0)/sum(E_high[D==0]==0)
    
    RR_true[i] <- ((sum(D==1 & E_high==1)/sum(E_high==1))/(sum(D==1 & E_high==0)/sum(E_high==0)))
    RR_obs[i] <- ((sum(D==1 & Eprime_high==1)/sum(Eprime_high==1))/(sum(D==1 & Eprime_high==0)/sum(Eprime_high==0)))
  }
  
  params <- data.frame(N, cutpt, a, b, sims, sigma, mean(Se_all), mean(Se_D1), mean(Se_D0), mean(Sp_all), mean(Sp_D1), mean(Sp_D0), mean(RR_true), mean(RR_obs))
  
  colnames(params) <- c("N", "cutoff", "a", "b", "Nsim", "SD", "Se all", "Se D=1", "Se D=0", "Sp all", "Sp D=1", "Sp D=0", "RR true", "RR obs")
  return(params)
}

set.seed(30308)

# case 1 is repeating the simulations from the paper 
# A. a = -4.5, b = 0.0019 
scenarioA1_100 <- flegal_sim(200, 1600, 2499, 2200, -4.5, 0.0019, 100, 200)
scenarioA1_300 <- flegal_sim(200, 1600, 2499, 2200, -4.5, 0.0019, 300, 200)
scenarioA1_500 <- flegal_sim(200, 1600, 2499, 2200, -4.5, 0.0019, 500, 200)
# B. a = -8.0, b = 0.0035
scenarioB1_100 <- flegal_sim(200, 1600, 2499, 2200, -8.0, 0.0035, 100, 200)
scenarioB1_300 <- flegal_sim(200, 1600, 2499, 2200, -8.0, 0.0035, 300, 200)
scenarioB1_500 <- flegal_sim(200, 1600, 2499, 2200, -8.0, 0.0035, 500, 200)
# C. a = -16.0, b = 0.0072
scenarioC1_100 <- flegal_sim(200, 1600, 2499, 2200, -16.0, 0.0072, 100, 200)
scenarioC1_300 <- flegal_sim(200, 1600, 2499, 2200, -16.0, 0.0072, 300, 200)
scenarioC1_500 <- flegal_sim(200, 1600, 2499, 2200, -16.0, 0.0072, 500, 200)
scenario1 <- rbind(scenarioA1_100, scenarioA1_300, scenarioA1_500, scenarioB1_100, scenarioB1_300, scenarioB1_500, scenarioC1_100, scenarioC1_300, scenarioC1_500)
```


#### Recreating original figures  

E true exposure values, 100 samples evenly spaced over specified interval  
E' measured exposure values: true + error term, where Error terms ~N with mean 0, "specified" SD  
true exposure values range integers 1,600 - 2,499, high/low cutoff of 2,200  
standard deviation of measurement error varied at 100, 300, 500  

They first look at misclassification close to cutpoints of 2,000 and 2,200. Figure 1 plots the measured exposure values (Y axis) against the true exposure values (X axis) at two cutpoints.

```{r fig1}
pop0 <- tibble(E = as.integer(seq(from = 1600, to = 2499, length.out = 100)), 
               Eerror = rnorm(100, mean = 0, sd = 150), 
               Eprime = E + Eerror, 
               E_high = ifelse(E > 2200, 1, 0), 
               Eprime_high = ifelse(Eprime > 2200,1, 0),
               E_mis = ifelse(E_high != Eprime_high, 1, 0), 
               D = rbinom(n = 100, size = 1, p = plogis(-10 + 0.004*E))
               )

sp1 <- ggplot(pop0, aes(x = E, y = Eprime)) + geom_point() + scale_x_continuous(limits = c(1400, 2600), breaks = seq(1400,2600, 200)) + scale_y_continuous(limits = c(1200, 2800), breaks = seq(1200,2800, 200)) + theme_bw()

Figure1a <- sp1 + labs(title = "Cutpoint of 2,000", x = "True Value", y = "Measured Value") + geom_vline(xintercept = 2000) + geom_hline(yintercept = 2000, linetype = "dashed") 
# Figure1a

Figure1b <- sp1 + labs(title = "Original scenario (n = 100)", x = "True Value", y = "Measured Value") + geom_vline(xintercept = 2200) + geom_hline(yintercept = 2200, linetype = "dashed")
# Figure1b
```

### Probability of misclassification and probability of disease
p probability of disease for each true exposure value, linear logistic model  ln(p/1-p) = a + b*E  
parameters a, b  
disease status 0 or 1 based on binomial n=1, prob of success = p from above  
Probability of misclassification is based on cumulative normal distribution. I based it on the difference between true value of E and the cutpoint; expected difference is 0.  

Figure 2 shows:  

1. expected probability of disease for a linear logistic model (a = -10, b = 0.004) AND  
2. probability of misclassification into low- and high- exposure based on normally distributed exposure measurement error (mean = 0, standard deviation = 150) 

Figure 3 shows: 

1. the expected probability of disease p as a quadratic function of the true exposure value E (p = 6.592 - 0.00673.E + 0.00000112E^2) over the specified range of values AND  
2. probability of misclassification (as in fig 2) 

```{r probD}
# expected probability of disease 
prob_D <- glm(D ~ E, data = pop0, family = binomial)
expected_pD <- predict(prob_D, pop0, type = "response")

# probability of misclassification
pop0$E_diff <- abs(pop0$E-2200)
pop0$a <- 1 - pnorm(pop0$E_diff, 0, 150)
# ggplot(pop, aes(x = E, y = a))+ geom_line() + geom_vline(xintercept = 2200) + theme_bw()

# Figure 2
flegal_figure2 <- ggplot(pop0, aes(x = E)) + geom_line(aes(y = expected_pD)) + geom_line(aes(y = a), linetype = 2) + geom_vline(xintercept = 2200) + theme_bw() + labs(title = "Fig 2. Prob of misclassification (dashed) & prob of disease (solid)", x = "True Value", y = "Probability")
flegal_figure2

# expected probability of disease p as a quadratic function of E
pop0$expected_pD_quad <- 6.592 - 0.00673*pop0$E + 0.00000172*(pop0$E**2)

# Figure 3:
flegal_figure3 <- ggplot(pop0, aes(x = E)) + geom_line(aes(y = expected_pD_quad)) + geom_line(aes(y = a), linetype = 2) + geom_vline(xintercept = 2200) + theme_bw() + labs(title = "Fig 3. Prob of misclassification (dashed) & prob of disease, quadratic term (solid)", x = "True Value", y = "Probability")

# probability of misclassification, varied SD to match simulation conditions
pop0$a100 <- 1 - pnorm(pop0$E_diff, 0, 100)
pop0$a300 <- 1 - pnorm(pop0$E_diff, 0, 300)
pop0$a500 <- 1 - pnorm(pop0$E_diff, 0, 500)

ggplot(pop0, aes(x = E)) + geom_line(aes(y = expected_pD), color = "darkgrey") + geom_line(aes(y = a), linetype = 2) + geom_line(aes(y = a100), color = "green") + geom_line(aes(y = a300), color = "green3") + geom_line(aes(y = a500), color = "darkgreen") + geom_vline(xintercept = 2200) + theme_bw() + labs(title = "Prob of misclassification & prob of disease (solid)", x = "True Value", y = "Probability")

```
The green lines are the expected probabilities of misclassification. 

Flegal et al. plot the probability of misclassification as a function of the distribution of the measurement error; not sure that it is actually the probability of misclassification? 
Why did they do that?? I think it may be more appropriate to simulate it and maybe re-create the population 100 times then get the proportion who were classified for each value of the exposure. Idk.

